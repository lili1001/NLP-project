{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import text from strDirc and generate raw data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDataList=[]\n",
    "strDircPath=r'C:\\Users\\admin\\OneDrive\\Documents\\supervised project\\processedFiles\\strDirc'\n",
    "processedDircPath=r'C:\\Users\\admin\\OneDrive\\Documents\\supervised project\\processedFiles'\n",
    "with open(os.path.join(processedDircPath,'rawDataList.txt'),'r') as rawDataFile:\n",
    "    rawDataList=json.loads(rawDataFile.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(text))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        text = np.char.replace(text, symbols[i], ' ')\n",
    "        text = np.char.replace(text, \"  \", \" \")\n",
    "#     data = np.char.replace(data, ',', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = word_tokenize(text)\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = convert_lower_case(text)\n",
    "    text = remove_punctuation(text) #remove comma seperately\n",
    "    text = remove_apostrophe(text)\n",
    "    text = remove_stop_words(text)\n",
    "    text = lemmatize(text)\n",
    "    text = text.rstrip('\\n')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1\n",
      "processing 2\n",
      "processing 3\n",
      "processing 4\n",
      "processing 5\n",
      "processing 6\n",
      "processing 7\n",
      "processing 8\n",
      "processing 9\n",
      "processing 10\n",
      "processing 11\n",
      "processing 12\n",
      "processing 13\n",
      "processing 14\n",
      "processing 15\n",
      "processing 16\n",
      "processing 17\n",
      "processing 18\n",
      "processing 19\n",
      "processing 20\n",
      "processing 21\n",
      "processing 22\n",
      "processing 23\n",
      "processing 24\n",
      "processing 25\n",
      "processing 26\n",
      "processing 27\n",
      "processing 28\n",
      "processing 29\n",
      "processing 30\n",
      "processing 31\n",
      "processing 32\n",
      "processing 33\n",
      "processing 34\n",
      "processing 35\n",
      "processing 36\n",
      "processing 37\n",
      "processing 38\n",
      "processing 39\n",
      "processing 40\n",
      "processing 41\n",
      "processing 42\n",
      "processing 43\n",
      "processing 44\n",
      "processing 45\n",
      "processing 46\n",
      "processing 47\n",
      "processing 48\n",
      "processing 49\n",
      "processing 50\n",
      "processing 51\n",
      "processing 52\n",
      "processing 53\n",
      "processing 54\n",
      "processing 55\n",
      "processing 56\n",
      "processing 57\n",
      "processing 58\n",
      "processing 59\n",
      "processing 60\n",
      "processing 61\n",
      "processing 62\n",
      "processing 63\n",
      "processing 64\n",
      "processing 65\n"
     ]
    }
   ],
   "source": [
    "cleanedList=[]\n",
    "counter=1\n",
    "for doc in rawDataList:\n",
    "    print('processing', counter)\n",
    "    processedDoc=preprocess(doc)\n",
    "    cleanedList.append(processedDoc)\n",
    "    counter+=1\n",
    "    \n",
    "with open(os.path.join(processedDircPath,'cleanedDataList.txt'),'w',encoding='utf-8') as cleanedDataFile:\n",
    "    cleaned=json.dumps(cleanedList)\n",
    "    cleanedDataFile.write(cleaned)\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
